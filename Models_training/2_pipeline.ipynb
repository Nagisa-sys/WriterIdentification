{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages Importation and parameters specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Clusterer, Global_feature_extractor, Local_features_extractor, Norms\n",
    "import Image, PCA_reduction, Distances, Encoder_NN\n",
    "from Dataset_loader import load_dataset\n",
    "from Accuracy import accuracy_optimised, accuracy\n",
    "import os, cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_PATHS = {\n",
    "    \"local_patch_extraction_representation\":[(\"SIFT\", \"SIFT\"),\n",
    "                                             (\"A-KAZE\", \"A-KAZE\"),\n",
    "                                             (\"SIFT\", \"Autoencoder\"),\n",
    "                                             (\"A-KAZE\", \"Autoencoder\")],\n",
    "    \"codebook_generation\": [\"MiniBatchKMeans\", \"KMedoids\"],\n",
    "    \"feature_encoding_and_pooling\": [\"BoVW\", \"VLAD\"],\n",
    "    \"dimentionality_reduction\": [None, \"PCA\"]\n",
    "}\n",
    "DATASETS = [(\"IAM\", None), (\"TrigraphSlant\", False), (\"TrigraphSlant\", True), (\"ICDAR\", \"en\"), (\"ICDAR\", \"ar\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipline = [0, 0, 0, 0]\n",
    "\n",
    "training_session = {\n",
    "    \"id\": \"Madoka\",\n",
    "    \"datasets\": [0],\n",
    "    \"training_size\": 10000,\n",
    "    \"testing_size\": 10000\n",
    "}\n",
    "\n",
    "if not os.path.exists(training_session[\"id\"]):\n",
    "  os.mkdir(training_session[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_big_set, test_big_set = list(), list()\n",
    "\n",
    "for choice in training_session[\"datasets\"]:\n",
    "    train_mini_set, test_mini_set = load_dataset(dataset=DATASETS[choice][0],\n",
    "                                                 path=\"./dataset\", \n",
    "                                                 size_train=training_session[\"training_size\"], \n",
    "                                                 size_test=training_session[\"testing_size\"],\n",
    "                                                 parametre=DATASETS[choice][1])\n",
    "    train_big_set.extend(train_mini_set)\n",
    "    test_big_set.extend(test_mini_set)\n",
    "    \n",
    "_, _, images_train_set = map(list, zip(*train_big_set))\n",
    "writers_test_set, images_names_test_set, images_test_set = map(list, zip(*test_big_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training images:\",len(images_train_set))\n",
    "print(\"Number of testing images:\",len(images_test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing local descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_chosen = PIPELINE_PATHS[\"local_patch_extraction_representation\"][pipline[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modules_chosen == (\"SIFT\", \"SIFT\"):\n",
    "  norm = Norms.Norm.hellinger_normalization\n",
    "  algo = cv2.xfeatures2d.SIFT_create()\n",
    "  local_features_extractor_descriptor = Local_features_extractor.Local_feature_extractor(algorithm=algo, norm=norm)\n",
    "elif modules_chosen == (\"A-KAZE\", \"A-KAZE\"):\n",
    "  norm = Norms.Norm.hellinger_normalization\n",
    "  algo = cv2.AKAZE_create()\n",
    "  local_features_extractor_descriptor = Local_features_extractor.Local_feature_extractor(algorithm=algo, norm=norm)\n",
    "else:\n",
    "    shape_images = '?'\n",
    "    max_key_points = '?'\n",
    "    model_path = training_session[\"id\"]+'/?.h5'\n",
    "    if modules_chosen[0] == \"SIFT\":\n",
    "      local_features_detector = cv2.xfeatures2d.SIFT_create()\n",
    "    elif modules_chosen[0] == \"A-KAZE\":\n",
    "      local_features_detector = cv2.AKAZE_create()\n",
    "    encoder = Encoder_NN.Encoder_NN((shape_images*2, shape_images*2),\n",
    "                                     max_key_points, \n",
    "                                     local_features_detector=local_features_detector)\n",
    "    encoder.set_model(model_path=model_path)\n",
    "    local_features_extractor_descriptor = Local_features_extractor.Local_feature_extractor(algorithm=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptors(local_features_extractor_descriptor, images_train_set, mini_size_sample=550):\n",
    "    images_pre_clustering = [Image.Image(image, local_feature_extractor=local_features_extractor_descriptor) \n",
    "                             for image \n",
    "                             in images_train_set]\n",
    "    list_local_descriptors = []\n",
    "    list_local_descriptors_all = []\n",
    "    for image in images_pre_clustering:\n",
    "        mini_list_local_descriptors = np.array(image.local_descriptors)\n",
    "        #repeated two times to ensure that each image can offer the mini_size sample\n",
    "        list_local_descriptors_all.extend(\n",
    "            mini_list_local_descriptors[\n",
    "                np.random.choice(\n",
    "                    mini_list_local_descriptors.shape[0], \n",
    "                    len(mini_list_local_descriptors), \n",
    "                    replace=False)\n",
    "            ]\n",
    "        )\n",
    "        list_local_descriptors.extend(\n",
    "            mini_list_local_descriptors[\n",
    "                np.random.choice(\n",
    "                    mini_list_local_descriptors.shape[0], \n",
    "                    min(mini_size_sample,len(mini_list_local_descriptors)), \n",
    "                    replace=False)\n",
    "            ]\n",
    "        )\n",
    "    return list_local_descriptors_all, list_local_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_all, descriptors_sample = get_descriptors(local_features_extractor_descriptor, images_train_set)\n",
    "print(len(descriptors_sample))\n",
    "print(len(descriptors_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searchig for the optimal value of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_algo = PIPELINE_PATHS[\"codebook_generation\"][pipline[1]]\n",
    "max_no_improvement = 500\n",
    "test_values=range(2, 400, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clusterer.Clusterer.choose_number_clusters_clustering(vectors=descriptors_sample, \n",
    "                                                      algo=clustering_algo,\n",
    "                                                      max_no_improvement=max_no_improvement, \n",
    "                                                      test_values=test_values,\n",
    "                                                      verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the accuracy of the system as a function of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_patch_representation = PIPELINE_PATHS[\"local_patch_extraction_representation\"][pipline[0]][1]\n",
    "if local_patch_representation==\"Autoencoder\":\n",
    "  distance_metric = Distances.Distance.angular_distance\n",
    "  accuracy_calculator = accuracy\n",
    "else:\n",
    "  distance_metric = Distances.Distance.chi2_distance\n",
    "  accuracy_calculator = accuracy_optimised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def principal_components(images_pre, pca_model_path, percentage_variance = 0.98):\n",
    "    global_descriptors = [image.global_descriptor for image in images_pre]\n",
    "    PCA_reduction.PCA_reduction.plot_variance_nbComponents(\n",
    "        vectors=global_descriptors, \n",
    "        percentage_variance=percentage_variance)\n",
    "    PCA_reduction.PCA_reduction.create_new_pca_model(vectors=global_descriptors, \n",
    "                                                    path_to_save=pca_model_path, \n",
    "                                                    percentage_variance=percentage_variance)\n",
    "\n",
    "    pca_instance = PCA_reduction.PCA_reduction(pca_model_path)\n",
    "    return pca_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_values=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_nb_clusters = []\n",
    "module_chosen = PIPELINE_PATHS[\"feature_encoding_and_pooling\"][pipline[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_pre = [Image.Image(image,image_name=image_name,local_feature_extractor=local_features_extractor_descriptor) \n",
    "              for image, image_name \n",
    "              in zip(images_test_set,images_names_test_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nb_clusters in set_nb_clusters:\n",
    "    clusters_centers_path = training_session[\"id\"]+\"/Centers_clusters_\"+str(nb_clusters)+\"nb.npy\"\n",
    "    Clusterer.Clusterer.fit_new_trainig(vectors=descriptors_all,\n",
    "                                        algo= clustering_algo,\n",
    "                                        path_to_save=clusters_centers_path,\n",
    "                                        nb_clusters=nb_clusters, \n",
    "                                        max_no_improvement=max_no_improvement,\n",
    "                                        metric=None,\n",
    "                                        verbose=0)\n",
    "    if module_chosen == \"BoVW\":\n",
    "      global_feature_extractor = Global_feature_extractor.BOW(clusters_centers_path=clusters_centers_path)\n",
    "    elif module_chosen == \"VLAD\":\n",
    "      global_feature_extractor = Global_feature_extractor.VLAD(clusters_centers_path=clusters_centers_path)\n",
    "    \n",
    "    [image.set_global_descriptor(global_feature_extractor) for image in images_pre]\n",
    "    \n",
    "    if (PIPELINE_PATHS[\"dimentionality_reduction\"][pipline[3]] == \"PCA\") and (module_chosen == \"VLAD\"):\n",
    "        percentage_variance = 0.95\n",
    "        pca_model_path = training_session[\"id\"]+\"/pca_model_\"+str(nb_clusters)+\"clusters.pkl\"\n",
    "        pca_instance = principal_components(images_pre, pca_model_path, percentage_variance)\n",
    "        global_feature_extractor = Global_feature_extractor.VLAD(clusters_centers_path=clusters_centers_path, \n",
    "                                                                 pca_instance=pca_instance)\n",
    "        [image.set_global_descriptor(global_feature_extractor) for image in images_pre]\n",
    "    \n",
    "    accuracy_values.append({nb_clusters:accuracy_calculator(X_test=images_pre, \n",
    "                                     Y_test=writers_test_set,\n",
    "                                     distance_metric=distance_metric)\n",
    "                           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
